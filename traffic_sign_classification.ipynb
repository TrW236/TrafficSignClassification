{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load pickled data\n",
    "import pickle\n",
    "def load_data_X():\n",
    "    training_file = 'data/train.p'\n",
    "    validation_file= 'data/valid.p'\n",
    "    testing_file = 'data/test.p'\n",
    "\n",
    "    with open(training_file, mode='rb') as f:\n",
    "        train = pickle.load(f)\n",
    "    with open(validation_file, mode='rb') as f:\n",
    "        valid = pickle.load(f)\n",
    "    with open(testing_file, mode='rb') as f:\n",
    "        test = pickle.load(f)\n",
    "\n",
    "    X_train = train['features']\n",
    "    X_valid = valid['features']\n",
    "    X_test = test['features']\n",
    "    print(\"Training data: \", X_train.shape)\n",
    "    print('Validation data: ',X_valid.shape)\n",
    "    print(\"Test data: \", X_test.shape)\n",
    "    return X_train, X_valid, X_test\n",
    "\n",
    "def load_data_y():\n",
    "    training_file = 'data/train.p'\n",
    "    validation_file= 'data/valid.p'\n",
    "    testing_file = 'data/test.p'\n",
    "    with open(training_file, mode='rb') as f:\n",
    "        train = pickle.load(f)\n",
    "    with open(validation_file, mode='rb') as f:\n",
    "        valid = pickle.load(f)\n",
    "    with open(testing_file, mode='rb') as f:\n",
    "        test = pickle.load(f)\n",
    "    y_train = train['labels']\n",
    "    y_valid = valid['labels']\n",
    "    y_test = test['labels']\n",
    "    print(\"Training labels: \",y_train.shape)\n",
    "    print('Validation labels: ',y_valid.shape)\n",
    "    print(\"Test labels: \", y_test.shape)\n",
    "    return y_train, y_valid, y_test\n",
    "\n",
    "X_train, X_valid, X_test = load_data_X()\n",
    "y_train, y_valid, y_test = load_data_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_train = len(y_train)\n",
    "n_validation = len(y_valid)\n",
    "n_test = len(y_test)\n",
    "image_shape = X_train[0].shape\n",
    "n_classes = max(y_train) + 1\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "\n",
    "def random_show(X, y, n):\n",
    "    idxs = []\n",
    "    pics = []\n",
    "    titles = []\n",
    "    for i in range(n):\n",
    "        idx = random.randint(0, len(X))\n",
    "        idxs.append(idx)\n",
    "        pics.append(X[idx])\n",
    "        titles.append('idx:'+str(idx)+',label:'+str(y[idx]))\n",
    "    draw_pics(n,pics,titles=titles)\n",
    "\n",
    "random_show(X_train, y_train, 6)\n",
    "random_show(X_train, y_train, 6)\n",
    "random_show(X_train, y_train, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def explore_data(y_train, n_class):\n",
    "    counts = []\n",
    "    for i in range(n_class):\n",
    "        counts.append(np.sum(y_train == i))\n",
    "    plt.bar(range(43),counts)\n",
    "    \n",
    "explore_data(y_train, 43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pdb\n",
    "from utils import *\n",
    "\n",
    "def blur_img(img, kernel=(5,5), sigmax = 1):\n",
    "    imcopy = cv2.GaussianBlur(img, kernel, sigmax)\n",
    "    return imcopy\n",
    "\n",
    "def get_gray(img):  \n",
    "    imcopy = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    return imcopy\n",
    "\n",
    "def get_hls(img):\n",
    "    imcopy = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "    return imcopy\n",
    "\n",
    "def get_rgb(img):\n",
    "    imcopy = cv2.cvtColor(img, cv2.COLOR_HLS2RGB)\n",
    "    return imcopy\n",
    "\n",
    "def change_contrast(img, rate):  # [0 , 255]\n",
    "    imcopy = np.copy(img)\n",
    "    imcopy = (imcopy.astype(np.float32)-128) * rate + 128\n",
    "    imcopy = np.maximum(imcopy, 0)\n",
    "    imcopy = np.minimum(imcopy, 255)\n",
    "    return imcopy.astype(np.uint8)\n",
    "\n",
    "def strech_img(img):  # img must be gray \n",
    "    imcopy = cv2.equalizeHist(img)\n",
    "    return imcopy\n",
    "\n",
    "def crop_img(img, crop=2):\n",
    "    imcopy = img[crop:img.shape[0]-crop, crop:img.shape[1]-crop,:]\n",
    "    return imcopy\n",
    "\n",
    "# test preprocessing\n",
    "idx=random.randint(0, len(X_train))\n",
    "print('idx: ', idx)\n",
    "raw = X_train[idx]\n",
    "raw = crop_img(raw)\n",
    "print('shape: ', raw.shape)\n",
    "# blurred = blur_img(raw, kernel=(3,3), sigmax=0)\n",
    "# gray = get_gray(raw)\n",
    "hls = get_hls(raw)\n",
    "streched = strech_img(hls[:,:,1])  # only cha\n",
    "hls[:, :, 1] = streched\n",
    "rgb = get_rgb(hls)\n",
    "\n",
    "pics = [raw, streched, rgb]\n",
    "draw_pics(3, pics, cmap=[None, 'gray', None], titles=['raw', 'gray(streched)', 'color(streched)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocessing img\n",
    "\n",
    "def preprocess_img(img):\n",
    "    hls = get_hls(img)\n",
    "    hls[:,:,1] = strech_img(hls[:,:,1])\n",
    "    rgb = get_rgb(hls)\n",
    "    return rgb\n",
    "#     imcopy = get_gray(img)\n",
    "#     return imcopy.reshape((img.shape[0], img.shape[1],1))\n",
    "\n",
    "def norm_img(img):\n",
    "    imcopy = (img.astype(np.float32) - 128.0)/ 128.0\n",
    "    return imcopy\n",
    "\n",
    "def preprocess_img_all(imgs, norm=True):\n",
    "    outimgs=[]\n",
    "    for img in imgs:\n",
    "        imcopy = crop_img(img)  # crop img\n",
    "        imcopy = preprocess_img(imcopy)  # preprocess\n",
    "        if norm:\n",
    "            imcopy = norm_img(imcopy)  # normalize\n",
    "        outimgs.append(imcopy)\n",
    "    outimgs = np.array(outimgs)\n",
    "    print('preprocessed, shape: ', outimgs.shape)\n",
    "    return outimgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data augmentation\n",
    "\n",
    "def add_light(img, light):  # [0, 255]\n",
    "    imcopy=np.copy(img)\n",
    "    imcopy = imcopy.astype(np.float32) + light\n",
    "    imcopy = np.maximum(imcopy, 0)\n",
    "    imcopy = np.minimum(imcopy, 255)\n",
    "    return imcopy.astype(np.uint8)\n",
    "\n",
    "def change_rot(img, ang=5):\n",
    "    rot = imutils.rotate(img, 2*ang*np.random.uniform()-ang)\n",
    "    return rot\n",
    "\n",
    "def change_light(img, light=30, dir=0 ):\n",
    "    if dir == 0:  # bi direction\n",
    "        light = light*2*np.random.uniform() - light\n",
    "    elif dir == 1:  # only +\n",
    "        light = light*np.random.uniform()\n",
    "    elif dir == -1:  # only -\n",
    "        light = -light*np.random.uniform()\n",
    "    imcopy = add_light(img, light)\n",
    "    return imcopy\n",
    "\n",
    "def change_shear(img, shear=5):\n",
    "    rows,cols,ch = img.shape\n",
    "    pts1 = np.float32([[5,5],[20,5],[5,20]])\n",
    "    pt1 = 5+shear*2*np.random.uniform()-shear\n",
    "    pt2 = 20+shear*2*np.random.uniform()-shear\n",
    "    pts2 = np.float32([[pt1,5],[pt2,pt1],[5,pt2]])\n",
    "    shear_M = cv2.getAffineTransform(pts1,pts2)\n",
    "    imcopy = cv2.warpAffine(img,shear_M,(cols,rows))\n",
    "    return imcopy\n",
    "\n",
    "def change_img(img, n=30, ang=10, light=30, shear=5):\n",
    "    imgs=[np.copy(img)]\n",
    "    for i in range(n-1):\n",
    "        imcopy = change_rot(img, ang=ang)\n",
    "        l_img = np.mean(img)\n",
    "        if l_img <= 40.0:\n",
    "            imcopy = change_light(imcopy, light=light, dir=1)\n",
    "        if l_img >=215:\n",
    "            imcopy = change_light(imcopy, light=light, dir=-1)\n",
    "        else:\n",
    "            imcopy = change_light(imcopy, light=light, dir=0)\n",
    "        imcopy = change_shear(imcopy, shear=shear)\n",
    "        imgs.append(imcopy)\n",
    "    return imgs\n",
    "\n",
    "import imutils \n",
    "idx=random.randint(0, len(X_train))\n",
    "# dark: 6641:10\n",
    "# idx = 6641\n",
    "print('idx: ',idx)\n",
    "raw = X_train[idx]\n",
    "imgs = change_img(raw, ang=10, light=40, shear=2)\n",
    "draw_pics(10, imgs[0:10],titles=['raw']*10)\n",
    "draw_pics(10, imgs[10:20],titles=['raw']*10)\n",
    "draw_pics(10, imgs[20:30],titles=['raw']*10)\n",
    "\n",
    "imgs = preprocess_img_all(imgs, norm=False)\n",
    "draw_pics(10, imgs[0:10], titles=['processed']*10)\n",
    "draw_pics(10, imgs[10:20],titles=['processed']*10)\n",
    "draw_pics(10, imgs[20:30],titles=['processed']*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data augmentation \n",
    "def aug_data(X_train, y_train, n=10, uplim=3000):\n",
    "    X_aug = []\n",
    "    y_aug = []\n",
    "    counts = [0] * 43\n",
    "    for x ,y  in zip(X_train, y_train):\n",
    "        diff = -(np.sum(y_train == y) + counts[y]) + uplim\n",
    "        if diff > 0:\n",
    "            if diff > uplim//3:\n",
    "                nn = n\n",
    "            else:\n",
    "                nn = n//2\n",
    "            xs = change_img(x, n=nn, ang=10, light=40, shear=2)\n",
    "            X_aug.append(xs)\n",
    "            ys = np.repeat(y , nn)\n",
    "            y_aug.append(ys)\n",
    "            counts[y] += nn - 1\n",
    "        else:\n",
    "            X_aug.append(np.array([x]))\n",
    "            y_aug.append(np.array([y]))\n",
    "            counts[y] += 1\n",
    "    X_aug = np.concatenate(X_aug)\n",
    "    y_aug = np.concatenate(y_aug)\n",
    "    print('X_aug: ', len(X_aug))\n",
    "    print('y_aug: ', len(y_aug))\n",
    "    return X_aug, y_aug\n",
    "\n",
    "def aug_shuffle_data(X_train, y_train, n=5, uplim=3000, n_loop=10):\n",
    "    for i in range(n_loop):\n",
    "        X_train, y_train = shuffle(X_train,y_train)\n",
    "        X_train, y_train = aug_data(X_train, y_train, n=n, uplim=uplim)\n",
    "        print(i, 'n_loop')\n",
    "    X_train, y_train = shuffle(X_train,y_train)\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, X_test = load_data_X()\n",
    "y_train, y_valid, y_test = load_data_y()\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(X_train,y_train)\n",
    "\n",
    "X_train, y_train = aug_shuffle_data(X_train, y_train, n=4, uplim=2000, n_loop=3)\n",
    "\n",
    "X_train, y_train = shuffle(X_train,y_train)\n",
    "\n",
    "random_show(X_train, y_train, 6)\n",
    "random_show(X_train, y_train, 6)\n",
    "random_show(X_train, y_train, 6)\n",
    "random_show(X_train, y_train, 6)\n",
    "random_show(X_train, y_train, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = preprocess_img_all(X_train)\n",
    "X_valid = preprocess_img_all(X_valid)\n",
    "X_test = preprocess_img_all(X_test)\n",
    "explore_data(y_train, 43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(np.max(X_train), np.min(X_train), len(X_train))\n",
    "print(np.max(X_valid), np.min(X_valid), len(X_valid))\n",
    "print(np.max(X_test), np.min(X_test), len(X_test))\n",
    "print(np.max(y_train), np.min(y_train), len(y_train))\n",
    "print(np.max(y_valid), np.min(y_valid), len(y_valid))\n",
    "print(np.max(y_test), np.min(y_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def add_layer_conv(x_in, n_f, depth_in, size_f=5, mu=0, sigma=0.1, activation='relu', stride=1, padding='VALID', \n",
    "                   val_names=['conv_W', 'conv_b']):\n",
    "    # weights tensor\n",
    "    conv_W = tf.Variable(tf.truncated_normal(shape=(size_f, size_f, depth_in, n_f), mean = mu, stddev = sigma), \n",
    "                         name=val_names[0])\n",
    "    # bias vector\n",
    "    conv_b = tf.Variable(tf.zeros(n_f), name=val_names[1])\n",
    "    \n",
    "    print(conv_W)\n",
    "    print(conv_b)\n",
    "    \n",
    "    # conv net\n",
    "    conv = tf.nn.conv2d(x_in, conv_W, strides=[1, stride, stride, 1], padding=padding) + conv_b\n",
    "    # activation \n",
    "    if activation == 'relu':\n",
    "        conv = tf.nn.relu(conv)\n",
    "    else:\n",
    "        print('No RELU function.')\n",
    "    print('One conv layer generated.')\n",
    "    return conv\n",
    "\n",
    "def add_layer_pool(x_in, stride=2, size_k=2, padding='VALID'):\n",
    "    pool = tf.nn.max_pool(x_in, ksize=[1, size_k, size_k, 1], strides=[1, stride, stride, 1], padding=padding)\n",
    "    print('One max pooling layer generated.')\n",
    "    return pool\n",
    "\n",
    "def add_layer_fc(x_in, shape, mu=0, sigma=0.1, activation='relu', val_names=['fc_W', 'fc_b'], \n",
    "                 out_name='fc_out'):   # shape=(n_in, n_out) tuple\n",
    "    # weights matrix\n",
    "    fc_W = tf.Variable(tf.truncated_normal(shape=shape, mean = mu, stddev = sigma), name=val_names[0])\n",
    "    # bias vector\n",
    "    fc_b = tf.Variable(tf.zeros(shape[1]), name= val_names[1])\n",
    "    \n",
    "    print(fc_W)\n",
    "    print(fc_b)\n",
    "    \n",
    "    # fully connected net\n",
    "    if activation is None:\n",
    "        fc = tf.add(tf.matmul(x_in, fc_W), fc_b, name=out_name)\n",
    "        print(fc)\n",
    "    else:\n",
    "        fc = tf.add(tf.matmul(x_in, fc_W), fc_b)\n",
    "    # activation\n",
    "    if activation == 'relu':\n",
    "        fc = tf.nn.relu(fc)\n",
    "    else:\n",
    "        print('No RELU function.')\n",
    "    print('One fully connected layer generated.')\n",
    "    return fc\n",
    "\n",
    "def add_dropout(x_in, keep_prob):\n",
    "    out = tf.nn.dropout(x_in, keep_prob)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "\n",
    "def gen_neural_net(x_in, keep_prob):  # input (28, 28, 3)\n",
    "    conv1 = add_layer_conv(x_in, n_f=16, depth_in=3, val_names=['cW1', 'cb1'])  # out(24,24,16)\n",
    "    pool1 = add_layer_pool(conv1)  # out (12,12,16)\n",
    "    \n",
    "    conv2 = add_layer_conv(pool1, n_f=32, depth_in=16, val_names=['cW2', 'cb2'])  # out(8,8,32)\n",
    "    pool2 = add_layer_pool(conv2)  # out (4,4,32)\n",
    "    \n",
    "    conv3 = add_layer_conv(pool2, n_f=64, depth_in=32, padding='SAME', val_names=['cW3', 'cb3'])  # (4, 4, 64)\n",
    "    pool3 = add_layer_pool(conv3)  # (2, 2, 64)\n",
    "    \n",
    "    fc0 = flatten(pool3)  # 256\n",
    "    fc0 = add_dropout(fc0, keep_prob)\n",
    "    fc1 = add_layer_fc(fc0, shape=(256, 120), val_names=['fW1', 'fb1'])\n",
    "    fc1 = add_dropout(fc1, keep_prob)\n",
    "    fc2 = add_layer_fc(fc1, shape=(120, 84), val_names=['fW2', 'fb2'])\n",
    "    fc2 = add_dropout(fc2, keep_prob)\n",
    "    fc3 = add_layer_fc(fc2, shape=(84, 43), activation=None, val_names=['fW3', 'fb3'], out_name='logits')\n",
    "    return fc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the network\n",
    "tf.reset_default_graph()\n",
    "rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, (None, 28, 28, 3), name='X')\n",
    "y = tf.placeholder(tf.int32, (None), name='y')\n",
    "one_hot_y = tf.one_hot(y, 43)\n",
    "keep_prob = tf.placeholder(tf.float32, name='k_prob')\n",
    "\n",
    "logits = gen_neural_net(X, keep_prob)  # for logits feed_dict: X, keep_prob\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)  # logits -> softmax -> cross_entropy\n",
    "loss_operation = tf.reduce_mean(cross_entropy)  # loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)  # create Optimizer\n",
    "training_operation = optimizer.minimize(loss_operation)  # set optimizer to minimize a loss\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))  \n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "tf.add_to_collection('inputs', X)\n",
    "tf.add_to_collection('inputs', y)\n",
    "tf.add_to_collection('inputs', keep_prob)\n",
    "tf.add_to_collection('logits', logits)\n",
    "tf.add_to_collection('train_op', training_operation)  # train_op feed_dict: X,y,keep_prob(0.5)\n",
    "tf.add_to_collection('acc_op', accuracy_operation)  # acc_op feed_dict: X, y, keep_prob(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(X_data, y_data, accuracy_operation, BATCH_SIZE=128):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0.0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        # session run accuracy operation\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={X: batch_x, y: batch_y, keep_prob:1.0})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples\n",
    "\n",
    "acc_train=[]\n",
    "acc_valid=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def train_model(sess, X_train, y_train, train_op, acc_op, EPOCHS=10, keep=0.5, BATCH_SIZE = 128):\n",
    "    num_examples = len(X_train)\n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        t1 = time.time()\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            # session run training operation\n",
    "            sess.run(train_op, feed_dict={X: batch_x, y: batch_y, keep_prob:keep})\n",
    "        train_acc = evaluate(X_train, y_train, acc_op)  # accuracy_operation\n",
    "        validation_accuracy = evaluate(X_valid, y_valid, acc_op) # accuracy_operation\n",
    "        t2 = time.time()\n",
    "        acc_train.append(train_acc)\n",
    "        acc_valid.append(validation_accuracy)\n",
    "        print(\"EPOCH {} \".format(i+1)+\"; Training Accuracy:  {:.3f}\".format(train_acc)+\n",
    "              \"; Validation Accuracy: {:.3f}\".format(validation_accuracy)+\"; Used time: {:.3f} s\".format(t2-t1))\n",
    "        print()\n",
    "\n",
    "    saver.save(sess, './mynet')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "# training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_model(sess, X_train, y_train, training_operation, accuracy_operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_save_accs(acc_train, acc_valid):\n",
    "    plt.plot(acc_train,'b',label ='Training Accuracy')\n",
    "    plt.plot(acc_valid,'g',label ='Validation Accuracy')\n",
    "    plt.xlabel('EPOCHS')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([0.5, 1])\n",
    "    plt.legend(loc='best')\n",
    "    with open('accs.p', 'wb') as f:\n",
    "        pickle.dump((acc_train, acc_valid), f)\n",
    "        print('accs saved')\n",
    "\n",
    "plot_save_accs(acc_train, acc_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training the model from saved file\n",
    "tf.reset_default_graph()\n",
    "def get_accs():\n",
    "    with open('accs.p', 'rb') as f:\n",
    "        acc_train, acc_valid = pickle.load(f)\n",
    "        print('accs restored')\n",
    "    return acc_train, acc_valid\n",
    "acc_train, acc_valid = get_accs()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.import_meta_graph('./mynet.meta')\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "    X, y, keep_prob = tf.get_collection('inputs')\n",
    "    train_op = tf.get_collection('train_op')[0]\n",
    "    acc_op = tf.get_collection('acc_op')[0]\n",
    "    train_model(sess, X_train, y_train, train_op, acc_op)\n",
    "\n",
    "plot_save_accs(acc_train, acc_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, X_test = load_data_X()\n",
    "y_train, y_valid, y_test = load_data_y()\n",
    "random_show(X_test, y_test, 6)\n",
    "random_show(X_test, y_test, 6)\n",
    "random_show(X_test, y_test, 6)\n",
    "print('number of data to test: ', len(X_test))\n",
    "print('number of data to test: ', len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = preprocess_img_all(X_test)\n",
    "explore_data(y_test, 43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from copy import deepcopy\n",
    "\n",
    "def predict(logits, X_test):\n",
    "    sess = tf.get_default_session()\n",
    "    return sess.run(tf.argmax(logits, 1), feed_dict={X:X_test, keep_prob:1.0})\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.import_meta_graph('./mynet.meta')\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "    X, y, keep_prob = tf.get_collection('inputs')\n",
    "    logits = tf.get_collection('logits')[0]\n",
    "    train_op = tf.get_collection('train_op')[0]\n",
    "    acc_op = tf.get_collection('acc_op')[0]\n",
    "    print('Test accuracy (all test images): ', evaluate(X_test, y_test, acc_op))\n",
    "    pred_res = predict(logits, X_test[0:20])\n",
    "    true_res = deepcopy(y_test[0:20])\n",
    "    print(pred_res, ' -> predicted labels')\n",
    "    print(true_res, ' -> true labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('For these images: {} % accuracy.'.format(np.sum(pred_res == true_res) / len(pred_res)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "signnames = pd.read_csv('signnames.csv')\n",
    "signnames.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import cv2\n",
    "tstimgs_adrs = glob.glob('./tst_imgs/*.jpg')\n",
    "rawpics =[]\n",
    "for i in range(len(tstimgs_adrs)):\n",
    "    tst = mpimg.imread(tstimgs_adrs[i])\n",
    "    tst = cv2.resize(tst, (256,256))\n",
    "    rawpics.append(tst)\n",
    "    \n",
    "draw_pics(8, rawpics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "tstpics=[]\n",
    "for i in range(len(rawpics)):\n",
    "    tstpics.append(cv2.resize(rawpics[i], (32,32)))\n",
    "draw_pics(8,tstpics)\n",
    "tstpics = preprocess_img_all(tstpics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prediction\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.import_meta_graph('./mynet.meta')\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "    X, y, keep_prob = tf.get_collection('inputs')\n",
    "    logits = tf.get_collection('logits')[0]\n",
    "    prob_res = sess.run(tf.nn.top_k(tf.nn.softmax(logits), k=5), feed_dict={X: tstpics, keep_prob:1.0})\n",
    "    print(prob_res)\n",
    "    \n",
    "def visualize(pics, prob, pred):\n",
    "    n_examples = len(pics)\n",
    "    fig, ax = plt.subplots(n_examples, 2, figsize=(15,15))\n",
    "    \n",
    "    for i in range(n_examples) :\n",
    "        labels = pred[i]\n",
    "        names = [signnames.iloc[l]['SignName'] for l in labels]\n",
    "        bars = np.arange(5)[::-1]\n",
    "        ax[i,0].imshow(pics[i])\n",
    "        ax[i,0].axis('off')\n",
    "        ax[i,1].barh(bars, prob[i])\n",
    "        ax[i,1].set_yticks(bars)\n",
    "        ax[i,1].set_yticklabels(names)\n",
    "        ax[i,1].yaxis.tick_right()\n",
    "        ax[i,1].set_xlim([0,1])\n",
    "    fig.tight_layout()\n",
    "    \n",
    "prob = prob_res[0]\n",
    "pred = prob_res[1]\n",
    "visualize(rawpics, prob, pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
